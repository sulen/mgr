{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T17:29:13.539368Z",
     "start_time": "2024-08-26T17:29:08.756762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 1: Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "id": "5da5dd9a60546dba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\magic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\magic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T17:30:54.188448Z",
     "start_time": "2024-08-26T17:30:51.992023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "# Example data\n",
    "train=pd.read_csv(\"data/dataset.csv\",encoding='latin1')\n",
    "train['text'] = train.OriginalTweet\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "def classes_def(x):\n",
    "    if x ==  \"Extremely Positive\":\n",
    "        return \"2\"\n",
    "    elif x == \"Extremely Negative\":\n",
    "        return \"0\"\n",
    "    elif x == \"Negative\":\n",
    "        return \"0\"\n",
    "    elif x ==  \"Positive\":\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"1\"\n",
    "\n",
    "\n",
    "train['label']=train['Sentiment'].apply(lambda x:classes_def(x))\n",
    "#Remove Urls and HTML links\n",
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "train['text_new']=train['text'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "train['text']=train['text_new'].apply(lambda x:remove_html(x))\n",
    "\n",
    "# Lower casing\n",
    "def lower(text):\n",
    "    low_text= text.lower()\n",
    "    return low_text\n",
    "train['text_new']=train['text'].apply(lambda x:lower(x))\n",
    "\n",
    "\n",
    "# Number removal\n",
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove\n",
    "train['text']=train['text_new'].apply(lambda x:remove_num(x))\n",
    "\n",
    "#Remove stopwords & Punctuations\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "train['text_new']=train['text'].apply(lambda x:punct_remove(x))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "train['text']=train['text_new'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "#Remove mentions and hashtags\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "train['text_new']=train['text'].apply(lambda x:remove_mention(x))\n",
    "\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "train['text']=train['text_new'].apply(lambda x:remove_hash(x))\n",
    "\n",
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove\n",
    "train['text_new']=train['text'].apply(lambda x:remove_space(x))\n",
    "train = train.drop(columns=['text_new'])\n",
    "\n",
    "stop_words = ['a', 'an', 'the']\n",
    "\n",
    "# Basic cleansing\n",
    "def cleansing(text):\n",
    "    # Tokenize\n",
    "    tokens = text.split(' ')\n",
    "    # Lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# All-in-one preproce\n",
    "def preprocess_x(x):\n",
    "    processed_x = [cleansing(text) for text in x]\n",
    "\n",
    "    return processed_x\n",
    "\n",
    "train['text_new']=train['text'].apply(lambda x:preprocess_x(x))\n",
    "X = train[\"text\"].tolist()\n",
    "y = train[\"label\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state = 177)\n"
   ],
   "id": "cec83fdc24539a97",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T17:31:32.308697Z",
     "start_time": "2024-08-26T17:31:04.358365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "\n",
    "# 3.1: Bag of Words\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_bow = count_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = count_vectorizer.transform(X_test)\n",
    "\n",
    "# 3.2: TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# 3.3: Word2Vec\n",
    "def get_word2vec_embeddings(texts, model, vector_size=100):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            embeddings.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            embeddings.append(np.zeros(vector_size))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=[word_tokenize(text) for text in X_train], vector_size=100, window=5, min_count=1, workers=8)\n",
    "X_train_w2v = get_word2vec_embeddings(X_train, word2vec_model)\n",
    "X_test_w2v = get_word2vec_embeddings(X_test, word2vec_model)\n",
    "\n",
    "# 3.4: GloVe\n",
    "def load_glove_embeddings(glove_file):\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    return glove_model\n",
    "\n",
    "def get_glove_embeddings(texts, glove_model, vector_size=300):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        vectors = [glove_model[token] for token in tokens if token in glove_model]\n",
    "        if vectors:\n",
    "            # Compute the mean of all word vectors for the sentence\n",
    "            embeddings.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            # In case no words in the text are found in the GloVe model, append a zero vector\n",
    "            embeddings.append(np.zeros(vector_size))\n",
    "    \n",
    "    # Ensure all elements in the list have the same size\n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "glove_file = 'glove.6B.300d.txt'\n",
    "glove_model = load_glove_embeddings(glove_file)\n",
    "X_train_glove = get_glove_embeddings(X_train, glove_model)\n",
    "X_test_glove = get_glove_embeddings(X_test, glove_model)"
   ],
   "id": "d9c3465aa246d4a7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T18:15:18.244134Z",
     "start_time": "2024-08-26T18:15:18.241133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# len(X_train_w2v[0])\n",
    "# len(X_train_glove)\n",
    "# X_train_tfidf.data\n",
    "# np.set_printoptions(suppress=True)\n",
    "len(X_train_tfidf[0, :].toarray().flatten())\n",
    "(X_train_tfidf[0, :].data)\n",
    "# (X_train_tfidf[0, :].toarray().flatten()[0:1000])\n",
    "# (X_train_glove)[0]\n",
    "# (X_train_tfidf).min()\n",
    "\n"
   ],
   "id": "b79f3ae6bc085d75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18990945, 0.20373031, 0.13510446, 0.13896817, 0.31317653,\n",
       "       0.19711124, 0.36803654, 0.15185621, 0.31078854, 0.1900012 ,\n",
       "       0.17566364, 0.21144662, 0.27247997, 0.22819204, 0.19109049,\n",
       "       0.36803654, 0.28788653])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T20:55:06.288922Z",
     "start_time": "2024-08-26T20:40:39.669504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Step 4: Model Training and Evaluation\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['l2'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    lr = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    \n",
    "    gs = GridSearchCV(lr, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Get the best parameters and best model\n",
    "    best_params = gs.best_params_\n",
    "    best_model = gs.best_estimator_\n",
    "\n",
    "    # Print best parameters\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    predictions = best_model.predict(X_test)\n",
    "    return classification_report(y_test, predictions)\n",
    "\n",
    "# Bag of Words\n",
    "accuracy_bow = train_and_evaluate(X_train_bow, y_train, X_test_bow, y_test)\n",
    "print(f'Bag of Words:\\n {accuracy_bow}')\n",
    "\n",
    "# TF-IDF\n",
    "accuracy_tfidf = train_and_evaluate(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f'TF-IDF:\\n {accuracy_tfidf}')\n",
    "\n",
    "# Word2Vec\n",
    "accuracy_w2v = train_and_evaluate(X_train_w2v, y_train, X_test_w2v, y_test)\n",
    "print(f'Word2Vec:\\n {accuracy_w2v}')\n",
    "\n",
    "# GloVe\n",
    "accuracy_glove = train_and_evaluate(X_train_glove, y_train, X_test_glove, y_test)\n",
    "print(f'GloVe:\\n {accuracy_glove}')"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best parameters: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Bag of Words:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      3451\n",
      "           1       0.71      0.72      0.71      1628\n",
      "           2       0.84      0.86      0.85      3912\n",
      "\n",
      "    accuracy                           0.81      8991\n",
      "   macro avg       0.80      0.80      0.80      8991\n",
      "weighted avg       0.81      0.81      0.81      8991\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best parameters: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82      3451\n",
      "           1       0.73      0.65      0.69      1628\n",
      "           2       0.83      0.86      0.85      3912\n",
      "\n",
      "    accuracy                           0.81      8991\n",
      "   macro avg       0.79      0.78      0.78      8991\n",
      "weighted avg       0.81      0.81      0.81      8991\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best parameters: {'C': 100, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Word2Vec:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.60      0.61      3451\n",
      "           1       0.53      0.37      0.43      1628\n",
      "           2       0.62      0.72      0.67      3912\n",
      "\n",
      "    accuracy                           0.61      8991\n",
      "   macro avg       0.59      0.56      0.57      8991\n",
      "weighted avg       0.60      0.61      0.60      8991\n",
      "\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "Best parameters: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "GloVe:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.70      3451\n",
      "           1       0.58      0.36      0.45      1628\n",
      "           2       0.69      0.77      0.73      3912\n",
      "\n",
      "    accuracy                           0.68      8991\n",
      "   macro avg       0.65      0.62      0.63      8991\n",
      "weighted avg       0.67      0.68      0.67      8991\n",
      "\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:54:12.422352Z",
     "start_time": "2024-08-15T12:54:12.419745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# without fine tuning\n",
    "def train_and_evaluate_no_ft(X_train, y_train, X_test, y_test):\n",
    "    best_model = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "    best_model.fit(X_train, y_train)\n",
    "    predictions = best_model.predict(X_test)\n",
    "    return classification_report(y_test, predictions)"
   ],
   "id": "ff5b53588a3bb34a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:58:13.362492Z",
     "start_time": "2024-08-15T12:58:04.903166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Bag of Words\n",
    "accuracy_bow = train_and_evaluate_no_ft(X_train_bow, y_train, X_test_bow, y_test)\n",
    "print(f'Bag of Words:\\n {accuracy_bow}')\n",
    "\n",
    "# TF-IDF\n",
    "accuracy_tfidf = train_and_evaluate_no_ft(X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(f'TF-IDF:\\n {accuracy_tfidf}')\n",
    "\n",
    "# Word2Vec\n",
    "accuracy_w2v = train_and_evaluate_no_ft(X_train_w2v, y_train, X_test_w2v, y_test)\n",
    "print(f'Word2Vec:\\n {accuracy_w2v}')\n",
    "\n",
    "# GloVe\n",
    "accuracy_glove = train_and_evaluate_no_ft(X_train_glove, y_train, X_test_glove, y_test)\n",
    "print(f'GloVe:\\n {accuracy_glove}')\n"
   ],
   "id": "74c7a59839140d67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81      3451\n",
      "           1       0.68      0.73      0.70      1628\n",
      "           2       0.84      0.84      0.84      3912\n",
      "\n",
      "    accuracy                           0.80      8991\n",
      "   macro avg       0.78      0.79      0.79      8991\n",
      "weighted avg       0.81      0.80      0.81      8991\n",
      "\n",
      "TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80      3451\n",
      "           1       0.73      0.59      0.66      1628\n",
      "           2       0.81      0.87      0.84      3912\n",
      "\n",
      "    accuracy                           0.79      8991\n",
      "   macro avg       0.78      0.75      0.76      8991\n",
      "weighted avg       0.79      0.79      0.79      8991\n",
      "\n",
      "Word2Vec:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.58      0.59      3451\n",
      "           1       0.54      0.32      0.40      1628\n",
      "           2       0.60      0.72      0.66      3912\n",
      "\n",
      "    accuracy                           0.60      8991\n",
      "   macro avg       0.58      0.54      0.55      8991\n",
      "weighted avg       0.59      0.60      0.59      8991\n",
      "\n",
      "GloVe:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70      3451\n",
      "           1       0.57      0.40      0.47      1628\n",
      "           2       0.69      0.76      0.73      3912\n",
      "\n",
      "    accuracy                           0.68      8991\n",
      "   macro avg       0.65      0.62      0.63      8991\n",
      "weighted avg       0.67      0.68      0.67      8991\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-11T11:20:37.589342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# 3.5 SFR-Embedding-Mistral\n",
    "def get_ember_embeddings(texts, tokenizer, model, max_length=128):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "        output = model(**encoded_input)\n",
    "        embeddings = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "model_name = \"Salesforce/SFR-Embedding-Mistral\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "X_train_ember = get_ember_embeddings(X_train.tolist(), tokenizer, model)\n",
    "X_test_ember = get_ember_embeddings(X_test.tolist(), tokenizer, model)\n",
    "\n",
    "accuracy_mistral = train_and_evaluate_no_ft(X_train_ember, y_train, X_test_ember, y_test)\n",
    "print(f'SFR-Embedding-Mistral:\\n {accuracy_mistral}')\n"
   ],
   "id": "8ccafac1cf311199",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T18:27:45.796091Z",
     "start_time": "2024-08-26T18:26:22.824406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# 3.5 SFR-Embedding-Mistral\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "# def get_ember_embeddings(texts, tokenizer, model, max_length=128):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "#         output = model(**encoded_input)\n",
    "#         embeddings = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#     return embeddings\n",
    "\n",
    "model_name = \"llmrails/ember-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "X_train_ember = get_ember_embeddings(X_train, tokenizer, model)\n",
    "# X_test_ember = get_ember_embeddings(X_test, tokenizer, model)\n",
    "# print(X_test)\n",
    "# print(X_train_ember)"
   ],
   "id": "95d3ae44d8829ced",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\mgr\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "2024-08-26 20:26:29,763 - INFO - Processed batch 1/72 in 2.03 seconds\n",
      "2024-08-26 20:26:30,885 - INFO - Processed batch 2/72 in 1.12 seconds\n",
      "2024-08-26 20:26:31,849 - INFO - Processed batch 3/72 in 0.96 seconds\n",
      "2024-08-26 20:26:32,837 - INFO - Processed batch 4/72 in 0.99 seconds\n",
      "2024-08-26 20:26:33,911 - INFO - Processed batch 5/72 in 1.07 seconds\n",
      "2024-08-26 20:26:35,005 - INFO - Processed batch 6/72 in 1.09 seconds\n",
      "2024-08-26 20:26:35,869 - INFO - Processed batch 7/72 in 0.86 seconds\n",
      "2024-08-26 20:26:36,792 - INFO - Processed batch 8/72 in 0.92 seconds\n",
      "2024-08-26 20:26:37,739 - INFO - Processed batch 9/72 in 0.95 seconds\n",
      "2024-08-26 20:26:38,944 - INFO - Processed batch 10/72 in 1.21 seconds\n",
      "2024-08-26 20:26:39,976 - INFO - Processed batch 11/72 in 1.03 seconds\n",
      "2024-08-26 20:26:40,918 - INFO - Processed batch 12/72 in 0.94 seconds\n",
      "2024-08-26 20:26:41,987 - INFO - Processed batch 13/72 in 1.07 seconds\n",
      "2024-08-26 20:26:42,947 - INFO - Processed batch 14/72 in 0.96 seconds\n",
      "2024-08-26 20:26:43,973 - INFO - Processed batch 15/72 in 1.03 seconds\n",
      "2024-08-26 20:26:45,147 - INFO - Processed batch 16/72 in 1.17 seconds\n",
      "2024-08-26 20:26:46,684 - INFO - Processed batch 17/72 in 1.54 seconds\n",
      "2024-08-26 20:26:47,650 - INFO - Processed batch 18/72 in 0.97 seconds\n",
      "2024-08-26 20:26:48,870 - INFO - Processed batch 19/72 in 1.22 seconds\n",
      "2024-08-26 20:26:50,074 - INFO - Processed batch 20/72 in 1.20 seconds\n",
      "2024-08-26 20:26:50,997 - INFO - Processed batch 21/72 in 0.92 seconds\n",
      "2024-08-26 20:26:51,885 - INFO - Processed batch 22/72 in 0.89 seconds\n",
      "2024-08-26 20:26:52,787 - INFO - Processed batch 23/72 in 0.90 seconds\n",
      "2024-08-26 20:26:54,316 - INFO - Processed batch 24/72 in 1.53 seconds\n",
      "2024-08-26 20:26:55,532 - INFO - Processed batch 25/72 in 1.22 seconds\n",
      "2024-08-26 20:26:56,593 - INFO - Processed batch 26/72 in 1.06 seconds\n",
      "2024-08-26 20:26:57,764 - INFO - Processed batch 27/72 in 1.17 seconds\n",
      "2024-08-26 20:26:58,880 - INFO - Processed batch 28/72 in 1.12 seconds\n",
      "2024-08-26 20:26:59,837 - INFO - Processed batch 29/72 in 0.96 seconds\n",
      "2024-08-26 20:27:00,795 - INFO - Processed batch 30/72 in 0.96 seconds\n",
      "2024-08-26 20:27:01,645 - INFO - Processed batch 31/72 in 0.85 seconds\n",
      "2024-08-26 20:27:02,587 - INFO - Processed batch 32/72 in 0.94 seconds\n",
      "2024-08-26 20:27:03,459 - INFO - Processed batch 33/72 in 0.87 seconds\n",
      "2024-08-26 20:27:04,503 - INFO - Processed batch 34/72 in 1.04 seconds\n",
      "2024-08-26 20:27:05,416 - INFO - Processed batch 35/72 in 0.91 seconds\n",
      "2024-08-26 20:27:06,389 - INFO - Processed batch 36/72 in 0.97 seconds\n",
      "2024-08-26 20:27:07,335 - INFO - Processed batch 37/72 in 0.95 seconds\n",
      "2024-08-26 20:27:08,619 - INFO - Processed batch 38/72 in 1.28 seconds\n",
      "2024-08-26 20:27:09,504 - INFO - Processed batch 39/72 in 0.88 seconds\n",
      "2024-08-26 20:27:10,468 - INFO - Processed batch 40/72 in 0.96 seconds\n",
      "2024-08-26 20:27:11,540 - INFO - Processed batch 41/72 in 1.07 seconds\n",
      "2024-08-26 20:27:12,621 - INFO - Processed batch 42/72 in 1.08 seconds\n",
      "2024-08-26 20:27:13,725 - INFO - Processed batch 43/72 in 1.10 seconds\n",
      "2024-08-26 20:27:14,956 - INFO - Processed batch 44/72 in 1.23 seconds\n",
      "2024-08-26 20:27:16,091 - INFO - Processed batch 45/72 in 1.14 seconds\n",
      "2024-08-26 20:27:17,207 - INFO - Processed batch 46/72 in 1.12 seconds\n",
      "2024-08-26 20:27:18,143 - INFO - Processed batch 47/72 in 0.94 seconds\n",
      "2024-08-26 20:27:19,058 - INFO - Processed batch 48/72 in 0.91 seconds\n",
      "2024-08-26 20:27:20,265 - INFO - Processed batch 49/72 in 1.21 seconds\n",
      "2024-08-26 20:27:21,313 - INFO - Processed batch 50/72 in 1.05 seconds\n",
      "2024-08-26 20:27:22,531 - INFO - Processed batch 51/72 in 1.22 seconds\n",
      "2024-08-26 20:27:23,415 - INFO - Processed batch 52/72 in 0.88 seconds\n",
      "2024-08-26 20:27:24,546 - INFO - Processed batch 53/72 in 1.13 seconds\n",
      "2024-08-26 20:27:25,871 - INFO - Processed batch 54/72 in 1.32 seconds\n",
      "2024-08-26 20:27:26,763 - INFO - Processed batch 55/72 in 0.89 seconds\n",
      "2024-08-26 20:27:28,076 - INFO - Processed batch 56/72 in 1.31 seconds\n",
      "2024-08-26 20:27:29,349 - INFO - Processed batch 57/72 in 1.27 seconds\n",
      "2024-08-26 20:27:30,964 - INFO - Processed batch 58/72 in 1.61 seconds\n",
      "2024-08-26 20:27:32,066 - INFO - Processed batch 59/72 in 1.10 seconds\n",
      "2024-08-26 20:27:33,008 - INFO - Processed batch 60/72 in 0.94 seconds\n",
      "2024-08-26 20:27:34,096 - INFO - Processed batch 61/72 in 1.09 seconds\n",
      "2024-08-26 20:27:35,204 - INFO - Processed batch 62/72 in 1.11 seconds\n",
      "2024-08-26 20:27:36,454 - INFO - Processed batch 63/72 in 1.25 seconds\n",
      "2024-08-26 20:27:37,423 - INFO - Processed batch 64/72 in 0.97 seconds\n",
      "2024-08-26 20:27:38,410 - INFO - Processed batch 65/72 in 0.99 seconds\n",
      "2024-08-26 20:27:39,434 - INFO - Processed batch 66/72 in 1.02 seconds\n",
      "2024-08-26 20:27:40,382 - INFO - Processed batch 67/72 in 0.95 seconds\n",
      "2024-08-26 20:27:41,311 - INFO - Processed batch 68/72 in 0.93 seconds\n",
      "2024-08-26 20:27:42,384 - INFO - Processed batch 69/72 in 1.07 seconds\n",
      "2024-08-26 20:27:43,594 - INFO - Processed batch 70/72 in 1.21 seconds\n",
      "2024-08-26 20:27:44,585 - INFO - Processed batch 71/72 in 0.99 seconds\n",
      "2024-08-26 20:27:45,760 - INFO - Processed batch 72/72 in 1.18 seconds\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T18:52:44.491741Z",
     "start_time": "2024-08-26T18:52:44.486741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_ember[0][0:900]\n",
    "# X_train_ember.max()"
   ],
   "id": "71b5e13bc4d2e11e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15651827,  0.04394898,  0.20617932,  0.23923256,  0.10326955,\n",
       "       -0.29402095,  0.39480647,  1.0215219 , -0.15108112,  0.8181932 ,\n",
       "        0.12444254, -0.5822182 ,  0.14245874, -0.03479619, -0.82616717,\n",
       "        0.12609835,  0.22270216, -0.45853424, -0.11734218,  0.33428392,\n",
       "        0.31397095, -0.28627127, -0.94127864, -0.0318335 , -0.7235342 ,\n",
       "        0.1517778 ,  0.44797742, -0.26657775,  1.0514002 ,  0.5815896 ,\n",
       "       -0.23330483,  0.2007327 ,  0.64061654, -1.1065009 ,  0.55198014,\n",
       "       -0.0770203 ,  1.0238037 , -0.56184906, -0.4664912 , -0.5129471 ,\n",
       "       -0.1803089 , -0.16969097,  0.93656695,  0.07370358, -0.21573305,\n",
       "       -0.21362841, -0.11025713, -0.22677784,  0.6886989 , -1.3100997 ,\n",
       "       -0.20212597,  0.45244542,  0.25122148, -0.20160519,  0.36776152,\n",
       "       -0.49760398, -0.5243846 , -0.06662889,  0.02018975,  0.66073227,\n",
       "        1.070978  , -0.21025403,  0.85591733, -1.2036958 ,  0.14897268,\n",
       "        0.20605506, -0.04492993, -0.12535998, -0.12822339, -0.19019064,\n",
       "        0.88389754,  0.17238724, -0.6378585 , -0.94460523,  0.2593571 ,\n",
       "       -0.0142855 , -0.1597095 ,  0.18864593,  0.3273353 ,  0.46242854,\n",
       "        0.73513484,  0.24256486,  0.02423774, -0.08857229, -0.870686  ,\n",
       "        0.14949825, -0.10329222,  0.84164476, -0.75558245, -0.3072537 ,\n",
       "        0.36759007,  0.48176998,  0.03077329, -0.30935964,  1.217676  ,\n",
       "        0.9933606 , -0.27402467,  0.5831999 ,  0.61400884, -0.6681308 ,\n",
       "       -0.30370986,  0.42955735,  0.4463135 ,  0.97054046, -0.21111295,\n",
       "        0.6029408 ,  0.91518325,  0.56802535, -0.29256162, -0.6051303 ,\n",
       "       -0.7936025 , -0.49414104,  0.1524775 , -0.0127049 , -0.14536123,\n",
       "        1.3130823 , -0.07317232,  0.4429978 , -0.9479855 ,  0.6016761 ,\n",
       "        1.1368116 , -0.07205082, -0.35951933, -0.39847282,  0.10859793,\n",
       "       -0.04497424,  0.60761404,  0.9861475 , -0.7070358 , -0.5173812 ,\n",
       "       -0.31956664,  0.06333695,  0.94824356,  0.3973382 ,  0.32821712,\n",
       "        0.78088236, -0.8846438 ,  0.62677354,  0.8328151 , -0.8013411 ,\n",
       "        0.27960703,  1.0481262 , -0.35005096,  1.0815096 , -0.70450187,\n",
       "        0.4415174 ,  0.33854523, -0.07101318, -0.20204706,  0.81789124,\n",
       "       -1.0769954 ,  0.3527312 , -0.13268574, -0.42585698, -0.302964  ,\n",
       "        0.31879565,  0.0949925 ,  0.44683075, -0.3654081 , -0.0391201 ,\n",
       "       -0.3024378 ,  0.25476652, -0.04574426,  0.5793543 , -0.48661518,\n",
       "        0.46016684, -0.36517408,  0.0265545 , -0.44814816, -0.4222568 ,\n",
       "        0.3114804 ,  0.47011012, -0.57641673,  0.10904347,  0.51476806,\n",
       "        0.34989166,  0.0558092 , -0.14770034,  0.4211227 , -0.16070879,\n",
       "        0.0272384 , -0.10996659,  0.08587878,  0.6379285 , -0.49468634,\n",
       "       -0.4731304 ,  0.30146235, -0.8468932 , -0.5111078 , -0.3591351 ,\n",
       "       -0.3378548 ,  0.2662069 ,  0.30117097,  0.11985356,  1.1367726 ,\n",
       "        0.6662073 , -0.31437817, -0.41094565, -0.18823312, -1.1352032 ,\n",
       "       -0.22307862,  1.020436  , -0.19585428,  0.45068395, -0.05904057,\n",
       "       -0.80007833,  0.26048884,  0.66788   , -0.93596077, -0.6346858 ,\n",
       "       -0.46203533,  0.45381388, -0.30479535, -0.81501555, -0.02889226,\n",
       "       -0.93901414, -0.48391148,  0.49260673, -0.2247072 , -0.31565014,\n",
       "       -0.13310067,  0.58396524,  0.41244456,  1.2785667 ,  0.5336226 ,\n",
       "        0.13417628,  0.10360278,  0.40849176, -0.6709244 , -0.2998072 ,\n",
       "        0.00184899,  0.61236244, -0.3792386 ,  1.1070954 ,  0.31398866,\n",
       "        0.5434449 ,  0.6914495 ,  0.04153585,  0.38510734,  0.12691839,\n",
       "        0.18894006,  0.80836314,  0.8417423 ,  0.785459  ,  0.10617904,\n",
       "        0.21580781,  0.5558547 ,  0.20201391,  0.05073497,  0.43788698,\n",
       "        0.29721344,  0.4210046 , -0.219106  ,  0.8296422 , -0.44780228,\n",
       "       -0.2965919 , -0.2199887 ,  0.2263382 ,  0.24729948, -0.36364153,\n",
       "        0.22875789, -0.3723187 ,  0.07521557, -0.51427716,  0.17228258,\n",
       "        0.39414144,  0.29841718,  1.2088606 , -0.09315519, -0.24267475,\n",
       "        0.0121086 , -0.25713262, -0.43568823, -0.10315145, -0.2745834 ,\n",
       "        0.44708344,  0.8528297 , -0.2236505 ,  0.03521314, -0.5076041 ,\n",
       "        0.15933369, -0.04682864,  0.23359919,  0.70868963,  0.46998227,\n",
       "        0.9137347 , -0.6423376 ,  0.23869076,  1.1121938 , -0.06354896,\n",
       "       -0.16224949, -0.19652304, -0.5925685 ,  0.48996168,  0.00461573,\n",
       "       -0.02085253, -0.15362273, -0.41600966, -0.5612328 , -0.39844835,\n",
       "       -0.29318115,  0.13460876, -0.06168355, -0.76782703, -0.3601217 ,\n",
       "        0.37980273,  0.69998085, -0.580641  , -0.06693372,  0.1610563 ,\n",
       "       -0.25767863,  0.13219748, -0.5986232 , -0.21124   , -0.731101  ,\n",
       "        0.17774472,  0.715191  ,  0.50854534, -0.581925  , -0.21262416,\n",
       "       -0.35167578, -0.47239083,  0.32109258, -0.1273735 , -0.15834925,\n",
       "        0.27822307,  0.49354112, -1.0937903 ,  1.0254052 , -0.89754426,\n",
       "       -0.15568271, -0.00155227, -0.37731475,  0.06385282,  0.60776204,\n",
       "        0.2799261 , -0.2352808 , -0.3290321 ,  0.03547854,  0.24842933,\n",
       "        0.8672033 ,  0.22425163,  0.19031721,  0.42027992, -0.3471674 ,\n",
       "       -0.28917623,  0.50935376,  0.3738608 , -0.23719963, -0.25359872,\n",
       "        0.39146152,  0.05068374,  0.4117695 ,  0.77394485, -0.10784659,\n",
       "        0.4777063 , -0.543279  ,  0.5001252 ,  0.6986623 ,  0.11012974,\n",
       "       -0.132049  ,  0.16214809, -0.32243022, -0.38122407, -0.656509  ,\n",
       "       -0.8021411 ,  0.5382842 , -0.4704626 ,  0.11625668, -0.22488822,\n",
       "       -0.0352462 , -0.9123765 ,  0.03348655,  0.10594705, -0.49160144,\n",
       "       -0.795654  ,  0.24785867, -0.03574538,  0.7136435 , -0.7058715 ,\n",
       "        0.44202188, -0.7413683 ,  0.6411703 ,  0.66361773,  0.31701237,\n",
       "        0.14861391, -1.28341   , -0.34265128, -0.4269068 ,  0.38803405,\n",
       "       -0.5034067 , -0.9931613 , -0.24138111,  0.07839812, -0.22921981,\n",
       "       -1.5015919 ,  0.2229412 ,  0.29413107,  0.51743996, -0.22081727,\n",
       "        0.32057965,  0.07687338,  0.24392222,  0.859119  , -0.3983686 ,\n",
       "        0.3709126 , -0.62578475,  0.40813655,  0.62718326, -0.23000515,\n",
       "       -0.01620952,  0.08143611, -0.9862499 ,  0.37543732, -0.8338365 ,\n",
       "       -0.25900742, -0.08571687,  0.38264272,  0.41328058,  0.07295749,\n",
       "       -0.9973182 , -0.5862624 ,  0.17701004,  0.37343064,  0.5872752 ,\n",
       "       -0.97748506, -0.54798776, -0.71335196,  0.30727813,  0.79937994,\n",
       "        0.558712  , -1.5170121 , -0.22929424, -0.17688033, -1.0011754 ,\n",
       "       -0.25576663,  0.40855396, -0.34366113,  0.16177827, -0.88285077,\n",
       "        0.350418  ,  0.18439461, -0.01678069, -0.31033927, -0.0098847 ,\n",
       "        0.5492561 , -0.02710344,  1.0046442 , -0.4949942 , -0.35625848,\n",
       "        0.5930344 , -0.1602367 ,  0.40370435, -0.66756237, -0.43043482,\n",
       "        0.06436256,  0.4455488 ,  0.7469313 ,  0.42714477, -0.541347  ,\n",
       "        0.54422235,  0.2824556 ,  0.14115047,  0.11103089, -0.47648555,\n",
       "        1.0513353 ,  0.27319682, -0.612135  ,  0.5771164 ,  0.04114656,\n",
       "       -0.6058597 ,  0.67930704,  0.34659916,  0.14964251,  0.60262495,\n",
       "        0.14370336,  0.64256287,  0.02908678, -0.22177674,  0.40392578,\n",
       "       -0.5363343 ,  0.14293066,  0.2807795 , -1.0356985 ,  0.1409452 ,\n",
       "       -1.1838248 , -0.00594944,  0.3028398 , -0.12430058,  0.22425307,\n",
       "        0.01683674,  0.11226443,  0.11830249,  0.1439713 ,  0.22515589,\n",
       "       -0.20453997, -0.14707151,  0.4895248 ,  0.31388396, -0.25109977,\n",
       "       -0.22046508, -0.03046682, -0.8934555 ,  0.57255614,  0.1138126 ,\n",
       "       -0.3043609 , -0.46006238, -0.03321499, -0.27988935,  0.760197  ,\n",
       "       -0.1402234 , -0.01077804,  0.03012733,  0.42128548,  0.53495055,\n",
       "        0.17471492, -0.19180745,  0.20424515, -0.8377201 , -0.22196218,\n",
       "        0.8006675 , -0.6156182 , -1.0417929 ,  0.7388181 , -0.22562179,\n",
       "        0.5551964 , -0.01108846,  0.36026314, -0.11385997, -1.2518725 ,\n",
       "       -0.54315245, -0.3692083 , -0.44483438, -0.8783788 ,  0.5051526 ,\n",
       "        0.23507775,  1.0952657 , -0.05974248,  0.3881358 ,  0.11450456,\n",
       "        0.5372351 ,  0.17618307,  0.24927445, -0.46960866, -0.19208553,\n",
       "       -0.66979957, -0.05439217,  0.39125356, -0.0008558 ,  0.35051286,\n",
       "       -0.08733795,  0.61102444,  0.3116801 , -0.14759257, -0.17015967,\n",
       "       -0.40864712, -1.0237381 ,  0.0678698 ,  1.1430304 ,  0.6632679 ,\n",
       "       -1.1191134 ,  0.7715584 , -0.55542403, -0.34509808, -0.28534004,\n",
       "       -0.12255077, -0.39404395, -0.62132686,  0.98828083, -0.28600922,\n",
       "        0.04611693, -0.0894366 ,  0.15615137,  0.05653272,  0.24846616,\n",
       "       -0.6648704 , -0.79319954, -0.88656515, -0.675473  ,  1.013157  ,\n",
       "       -0.16325903, -0.07144183, -0.5468438 , -0.53213036,  0.7874462 ,\n",
       "       -0.27234724,  0.25980538,  1.5077701 , -0.261491  , -0.06130876,\n",
       "       -0.6080894 ,  0.37540984, -0.13504812,  0.41576284,  0.35183468,\n",
       "       -0.7160619 , -0.14023449,  0.0722762 , -0.22511828, -0.26387554,\n",
       "       -0.43563375, -0.08829355,  0.6820243 ,  0.3521475 ,  0.8515861 ,\n",
       "       -0.9617487 , -0.45860657, -0.5221267 ,  0.05951724,  1.0981925 ,\n",
       "       -0.00133271,  1.0057571 ,  0.14442068,  0.24025317, -0.16516232,\n",
       "        0.51850253, -0.2657512 , -0.9205681 ,  0.42821154, -0.05809279,\n",
       "       -0.91364765,  0.16162993,  0.2251007 , -0.65212953, -0.620349  ,\n",
       "        0.04793337,  0.38690975,  0.1517591 , -0.3181467 , -0.3577036 ,\n",
       "       -0.77412134, -0.29773498,  0.7148419 ,  0.84001285, -0.19082338,\n",
       "        0.9759433 , -0.09741153, -0.06965185, -0.29055396, -0.22923581,\n",
       "        0.15214226,  0.173518  , -0.08769227, -1.0602005 ,  0.69864696,\n",
       "       -0.15573004,  0.12325887,  0.7626699 , -0.17492369, -0.5638484 ,\n",
       "        0.27807823, -0.14555758,  0.16563535,  0.06771549,  0.19867755,\n",
       "        0.26441318, -0.36800426, -0.7574132 , -0.6326999 ,  0.12220568,\n",
       "       -0.04690468,  0.4140698 , -0.5609831 , -0.56982934,  0.38879836,\n",
       "       -0.14849277, -0.02596247, -0.4495858 , -0.11731679, -0.8543171 ,\n",
       "       -0.55584764, -0.0261905 ,  0.02458436,  0.134463  ,  1.1561838 ,\n",
       "        0.2598132 ,  0.1759923 , -0.227941  ,  0.31787652, -0.26399323,\n",
       "       -0.08826012,  0.04897122,  0.26502723,  0.12378461, -0.08872802,\n",
       "       -0.69152266, -1.0197953 , -0.05199561, -0.7653294 , -0.3934418 ,\n",
       "        0.14120361, -0.49615356,  1.1756082 , -0.29933923,  0.30235735,\n",
       "       -0.00661759, -0.8826424 ,  0.06807122,  0.5526329 ,  0.14561844,\n",
       "       -0.2711557 ,  0.04586858, -0.5709272 , -0.17602922, -0.00941444,\n",
       "       -0.6873483 , -0.33608034, -1.022048  , -0.43207124,  0.42897618,\n",
       "       -0.5388283 , -0.73304284,  0.18711407, -0.38091674,  0.03364852,\n",
       "       -0.7490454 , -0.18625739,  0.33972484,  0.6309991 ,  0.3008974 ,\n",
       "        0.2391826 ,  0.30837286,  0.01217943, -0.08267929,  0.2970012 ,\n",
       "       -0.3359474 ,  0.4357046 ,  0.7213759 ,  0.14015152,  1.3658003 ,\n",
       "       -0.3316895 ,  0.03060848,  0.8162292 , -0.1976441 ,  0.16643246,\n",
       "       -0.11702208, -0.86869824, -0.4932511 , -0.41509336, -0.6514977 ,\n",
       "        0.4453919 ,  0.5671924 , -0.601357  ,  0.47834516, -0.8323408 ,\n",
       "       -0.72461146,  0.16457203, -0.9126622 , -0.47712147,  0.48091492,\n",
       "       -0.49858466,  0.56101865, -0.12988047, -0.3797361 , -0.09173443,\n",
       "       -0.17995365,  0.27132288, -0.38825536,  0.31726262, -0.36037078,\n",
       "        0.0153659 , -0.61078894, -0.19023927, -0.20372114,  0.7942501 ,\n",
       "       -0.6622666 , -0.21746616, -0.06281637, -0.07812682,  0.36549863,\n",
       "       -0.11661591, -0.43398327, -0.29696727, -0.0803662 , -0.5089137 ,\n",
       "       -0.18785952,  0.80767107, -1.0250782 ,  0.8937368 , -0.5117539 ,\n",
       "        0.3990163 , -0.7940069 ,  0.9001815 , -0.8770179 ,  0.1493736 ,\n",
       "       -0.61760867,  0.31553388,  0.15185611,  0.16869731,  0.60688806,\n",
       "        0.19398144,  0.08452465, -0.68195385,  0.21338317, -0.3439819 ,\n",
       "        1.3384445 ,  0.5099665 , -0.26336348, -0.35672185,  0.21418737,\n",
       "        0.4335005 , -1.1053535 , -0.5815785 ,  0.66753167,  0.00584458,\n",
       "       -0.21546674, -0.32639316, -0.19660948,  0.27524453, -0.25545666,\n",
       "        0.42923042, -0.01585205, -0.22048765, -0.75358695, -0.0829161 ,\n",
       "       -0.20930041,  1.2585111 , -0.28159556, -1.0063722 ,  0.37343743,\n",
       "       -0.24600324, -0.46426618,  1.1144993 , -0.6030689 ,  0.2680921 ,\n",
       "        0.7977193 , -0.08622591, -0.2646255 ,  0.6736338 , -0.06531058,\n",
       "        0.26179263, -0.131124  ,  0.15192671, -0.03152561, -0.20039697,\n",
       "        0.00923613, -0.40301144, -0.32459456,  0.8571595 , -0.24691263,\n",
       "       -0.15901756,  0.36351502,  0.22863373, -0.22776733, -0.7594883 ,\n",
       "        0.4472746 , -0.48326752, -0.50013685,  0.68464714, -0.2975162 ,\n",
       "       -0.2715715 ,  0.98951167,  0.0782628 ,  0.4470417 ,  0.6269345 ,\n",
       "        0.24016751,  0.01459972,  0.5166709 ,  0.10334514,  0.00421472,\n",
       "       -0.29939762, -0.13882637, -0.47663915, -0.24008326, -0.57972825,\n",
       "       -0.4270259 ,  0.63780665, -0.7160968 , -0.066879  ,  0.41948673,\n",
       "       -0.00516673, -0.8715301 , -0.5434186 , -0.6100576 ,  0.43253362,\n",
       "       -0.50200874, -0.17025368, -0.36200577,  0.1873661 , -0.36281404,\n",
       "        0.2856071 , -0.2651899 ,  0.42824307,  0.44482413, -0.36720803,\n",
       "        0.35913217,  0.04457159, -0.87635726,  0.12974586, -1.1516297 ,\n",
       "       -0.06984185,  0.7215665 , -1.098817  , -0.2319452 ,  0.28896233,\n",
       "        0.4539621 , -0.41266757, -0.7025234 , -0.39644125, -0.03636445],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFR-Embedding-Mistral:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71      3451\n",
      "           1       0.60      0.49      0.54      1628\n",
      "           2       0.71      0.74      0.72      3912\n",
      "\n",
      "    accuracy                           0.69      8991\n",
      "   macro avg       0.67      0.65      0.66      8991\n",
      "weighted avg       0.69      0.69      0.69      8991\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "\n",
    "# SFR-Embedding-Mistral\n",
    "accuracy_mistral = train_and_evaluate_no_ft(X_train_ember, y_train, X_test_ember, y_test)\n",
    "print(f'SFR-Embedding-Mistral:\\n {accuracy_mistral}')\n"
   ],
   "id": "640c6ee54a15d94e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:35:59.223546Z",
     "start_time": "2024-08-14T14:35:57.600207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "X_train_ember = get_ember_embeddings(['random text', 'very nice'], tokenizer, model)\n",
    "print(X_train_ember)\n"
   ],
   "id": "1c95f40bc6ee5be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\mgr\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "2024-08-14 16:35:59,220 - INFO - Processed batch 1/1 in 0.84 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.30708066 -0.38886583  0.5703759  ...  0.58131295  0.02450421\n",
      "  -0.905235  ]\n",
      " [ 0.07205359  0.41388914  0.47324252 ... -0.55121666 -0.04188719\n",
      "   0.7166905 ]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:38:49.820032Z",
     "start_time": "2024-08-14T14:37:19.705027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_ember = get_ember_embeddings(X_train, tokenizer, model)\n",
    "# print(X_train_mistral)\n"
   ],
   "id": "f630dd08eb1fbd80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 16:37:21,132 - INFO - Processed batch 1/72 in 1.42 seconds\n",
      "2024-08-14 16:37:22,441 - INFO - Processed batch 2/72 in 1.31 seconds\n",
      "2024-08-14 16:37:23,547 - INFO - Processed batch 3/72 in 1.11 seconds\n",
      "2024-08-14 16:37:24,696 - INFO - Processed batch 4/72 in 1.15 seconds\n",
      "2024-08-14 16:37:25,950 - INFO - Processed batch 5/72 in 1.25 seconds\n",
      "2024-08-14 16:37:27,214 - INFO - Processed batch 6/72 in 1.26 seconds\n",
      "2024-08-14 16:37:28,211 - INFO - Processed batch 7/72 in 1.00 seconds\n",
      "2024-08-14 16:37:29,273 - INFO - Processed batch 8/72 in 1.06 seconds\n",
      "2024-08-14 16:37:30,364 - INFO - Processed batch 9/72 in 1.09 seconds\n",
      "2024-08-14 16:37:31,773 - INFO - Processed batch 10/72 in 1.41 seconds\n",
      "2024-08-14 16:37:32,961 - INFO - Processed batch 11/72 in 1.19 seconds\n",
      "2024-08-14 16:37:34,060 - INFO - Processed batch 12/72 in 1.10 seconds\n",
      "2024-08-14 16:37:35,305 - INFO - Processed batch 13/72 in 1.24 seconds\n",
      "2024-08-14 16:37:36,438 - INFO - Processed batch 14/72 in 1.13 seconds\n",
      "2024-08-14 16:37:37,643 - INFO - Processed batch 15/72 in 1.20 seconds\n",
      "2024-08-14 16:37:39,014 - INFO - Processed batch 16/72 in 1.37 seconds\n",
      "2024-08-14 16:37:40,807 - INFO - Processed batch 17/72 in 1.79 seconds\n",
      "2024-08-14 16:37:41,923 - INFO - Processed batch 18/72 in 1.12 seconds\n",
      "2024-08-14 16:37:43,357 - INFO - Processed batch 19/72 in 1.43 seconds\n",
      "2024-08-14 16:37:44,765 - INFO - Processed batch 20/72 in 1.41 seconds\n",
      "2024-08-14 16:37:45,846 - INFO - Processed batch 21/72 in 1.08 seconds\n",
      "2024-08-14 16:37:46,864 - INFO - Processed batch 22/72 in 1.02 seconds\n",
      "2024-08-14 16:37:47,908 - INFO - Processed batch 23/72 in 1.04 seconds\n",
      "2024-08-14 16:37:49,693 - INFO - Processed batch 24/72 in 1.79 seconds\n",
      "2024-08-14 16:37:51,108 - INFO - Processed batch 25/72 in 1.42 seconds\n",
      "2024-08-14 16:37:52,348 - INFO - Processed batch 26/72 in 1.24 seconds\n",
      "2024-08-14 16:37:53,706 - INFO - Processed batch 27/72 in 1.36 seconds\n",
      "2024-08-14 16:37:55,023 - INFO - Processed batch 28/72 in 1.32 seconds\n",
      "2024-08-14 16:37:56,150 - INFO - Processed batch 29/72 in 1.13 seconds\n",
      "2024-08-14 16:37:57,276 - INFO - Processed batch 30/72 in 1.13 seconds\n",
      "2024-08-14 16:37:58,267 - INFO - Processed batch 31/72 in 0.99 seconds\n",
      "2024-08-14 16:37:59,354 - INFO - Processed batch 32/72 in 1.09 seconds\n",
      "2024-08-14 16:38:00,378 - INFO - Processed batch 33/72 in 1.02 seconds\n",
      "2024-08-14 16:38:01,600 - INFO - Processed batch 34/72 in 1.22 seconds\n",
      "2024-08-14 16:38:02,664 - INFO - Processed batch 35/72 in 1.06 seconds\n",
      "2024-08-14 16:38:03,806 - INFO - Processed batch 36/72 in 1.14 seconds\n",
      "2024-08-14 16:38:04,891 - INFO - Processed batch 37/72 in 1.08 seconds\n",
      "2024-08-14 16:38:06,375 - INFO - Processed batch 38/72 in 1.48 seconds\n",
      "2024-08-14 16:38:07,406 - INFO - Processed batch 39/72 in 1.03 seconds\n",
      "2024-08-14 16:38:08,515 - INFO - Processed batch 40/72 in 1.11 seconds\n",
      "2024-08-14 16:38:09,763 - INFO - Processed batch 41/72 in 1.25 seconds\n",
      "2024-08-14 16:38:11,008 - INFO - Processed batch 42/72 in 1.24 seconds\n",
      "2024-08-14 16:38:12,295 - INFO - Processed batch 43/72 in 1.29 seconds\n",
      "2024-08-14 16:38:13,849 - INFO - Processed batch 44/72 in 1.55 seconds\n",
      "2024-08-14 16:38:15,184 - INFO - Processed batch 45/72 in 1.33 seconds\n",
      "2024-08-14 16:38:16,459 - INFO - Processed batch 46/72 in 1.27 seconds\n",
      "2024-08-14 16:38:17,553 - INFO - Processed batch 47/72 in 1.09 seconds\n",
      "2024-08-14 16:38:18,615 - INFO - Processed batch 48/72 in 1.06 seconds\n",
      "2024-08-14 16:38:20,025 - INFO - Processed batch 49/72 in 1.41 seconds\n",
      "2024-08-14 16:38:21,250 - INFO - Processed batch 50/72 in 1.22 seconds\n",
      "2024-08-14 16:38:22,662 - INFO - Processed batch 51/72 in 1.41 seconds\n",
      "2024-08-14 16:38:23,704 - INFO - Processed batch 52/72 in 1.04 seconds\n",
      "2024-08-14 16:38:25,029 - INFO - Processed batch 53/72 in 1.33 seconds\n",
      "2024-08-14 16:38:26,574 - INFO - Processed batch 54/72 in 1.55 seconds\n",
      "2024-08-14 16:38:27,597 - INFO - Processed batch 55/72 in 1.02 seconds\n",
      "2024-08-14 16:38:29,113 - INFO - Processed batch 56/72 in 1.52 seconds\n",
      "2024-08-14 16:38:30,610 - INFO - Processed batch 57/72 in 1.50 seconds\n",
      "2024-08-14 16:38:32,474 - INFO - Processed batch 58/72 in 1.86 seconds\n",
      "2024-08-14 16:38:33,741 - INFO - Processed batch 59/72 in 1.27 seconds\n",
      "2024-08-14 16:38:34,842 - INFO - Processed batch 60/72 in 1.10 seconds\n",
      "2024-08-14 16:38:36,128 - INFO - Processed batch 61/72 in 1.28 seconds\n",
      "2024-08-14 16:38:37,425 - INFO - Processed batch 62/72 in 1.30 seconds\n",
      "2024-08-14 16:38:38,882 - INFO - Processed batch 63/72 in 1.46 seconds\n",
      "2024-08-14 16:38:39,994 - INFO - Processed batch 64/72 in 1.11 seconds\n",
      "2024-08-14 16:38:41,159 - INFO - Processed batch 65/72 in 1.16 seconds\n",
      "2024-08-14 16:38:42,372 - INFO - Processed batch 66/72 in 1.21 seconds\n",
      "2024-08-14 16:38:43,491 - INFO - Processed batch 67/72 in 1.12 seconds\n",
      "2024-08-14 16:38:44,584 - INFO - Processed batch 68/72 in 1.09 seconds\n",
      "2024-08-14 16:38:45,809 - INFO - Processed batch 69/72 in 1.23 seconds\n",
      "2024-08-14 16:38:47,230 - INFO - Processed batch 70/72 in 1.42 seconds\n",
      "2024-08-14 16:38:48,386 - INFO - Processed batch 71/72 in 1.15 seconds\n",
      "2024-08-14 16:38:49,787 - INFO - Processed batch 72/72 in 1.40 seconds\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "364d9f8405276d08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T18:25:18.959838Z",
     "start_time": "2024-08-26T18:25:15.199350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_ember_embeddings(texts, tokenizer, model, max_length=128, batch_size=500):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    total_batches = len(texts) // batch_size + (1 if len(texts) % batch_size != 0 else 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            start_time = time.time() \n",
    "            \n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "            encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "            with autocast():\n",
    "                output = model(**encoded_input)\n",
    "            batch_embeddings = output.last_hidden_state.mean(dim=1).cpu().numpy().astype(np.float32)\n",
    "            embeddings.append(batch_embeddings)\n",
    "            \n",
    "            end_time = time.time() \n",
    "            elapsed_time = end_time - start_time\n",
    "            \n",
    "            logging.info(f'Processed batch {i // batch_size + 1}/{total_batches} in {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    return np.vstack(embeddings)\n"
   ],
   "id": "2c995e072d75293e",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:23:53.385390Z",
     "start_time": "2024-08-14T14:23:52.843082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_ember = get_ember_embeddings(['random text', 'very nice'], tokenizer, model)\n",
    "# X_train_mistral = get_mistral_embeddings(X_train[:15], tokenizer, model)\n",
    "print(X_train_ember)\n"
   ],
   "id": "53217af2b108bcc3",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train_mistral \u001B[38;5;241m=\u001B[39m \u001B[43mget_mistral_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrandom text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvery nice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# X_train_mistral = get_mistral_embeddings(X_train[:15], tokenizer, model)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_train_mistral)\n",
      "Cell \u001B[1;32mIn[5], line 11\u001B[0m, in \u001B[0;36mget_mistral_embeddings\u001B[1;34m(texts, tokenizer, model, max_length, batch_size)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_mistral_embeddings\u001B[39m(texts, tokenizer, model, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m):\n\u001B[0;32m     10\u001B[0m     device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     13\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\transformers\\modeling_utils.py:2883\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[0;32m   2879\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2880\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2881\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2882\u001B[0m         )\n\u001B[1;32m-> 2883\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1170\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1171\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1173\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    778\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 779\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    789\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    778\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 779\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    789\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 779 (1 times)]\u001B[0m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    778\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 779\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    789\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 804\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    805\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    807\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32mS:\\mgr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1153\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1154\u001B[0m             device,\n\u001B[0;32m   1155\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1156\u001B[0m             non_blocking,\n\u001B[0;32m   1157\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1158\u001B[0m         )\n\u001B[1;32m-> 1159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:04:29.144400Z",
     "start_time": "2024-08-14T13:04:29.141827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "len(X_train[:100])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "id": "b6bbf334df598001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6af4abd1cf828b1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
